# Production Readiness Checklist

## Overview

Before running aggTrades ingestion in production (especially with n8n workflows), run these checks to prevent common failure modes.

## Pre-Flight Checks

### Quick Check
```bash
uv run python scripts/check_ingestion_ready.py \
    --db /media/sam/2TB-NVMe/liquidationheatmap_db/liquidations.duckdb \
    --data-dir /path/to/binance-data
```

**Exit codes**:
- `0`: All checks passed, safe to proceed
- `1`: One or more checks failed, fix before ingestion

---

## What Gets Checked

### 1. Concurrent Write Lock âœ…
**Problem Prevented**: Database corruption from simultaneous writes

**Check**: Tests if ingestion lock file is available
**Lock Path**: `/tmp/liquidation_heatmap_ingestion.lock`

**If Failed**:
```bash
# Find which process holds the lock
lsof /tmp/liquidation_heatmap_ingestion.lock

# Option 1: Wait for completion
# Option 2: Kill process (dangerous!)
```

**n8n Integration**: Add this check as first step in workflow

---

### 2. Disk Space ðŸ’¾
**Problem Prevented**: Mid-ingestion crash due to disk full

**Check**: Verifies â‰¥100GB free space on database disk
**Why 100GB**: ~300GB for 4 years + 100GB buffer

**If Failed**:
```bash
# Check current usage
df -h /media/sam/1TB

# Option 1: Free up space
# Option 2: Move DB to larger disk
# Option 3: Archive old data
```

**Estimation**:
- 1 year BTCUSDT: ~50-80GB
- 4 years: ~200-320GB
- Buffer: 50% extra recommended

---

### 3. Database Connectivity ðŸ”—
**Problem Prevented**: Connection errors, lock conflicts

**Check**: Attempts to connect and count existing rows

**If Failed (Lock Conflict)**:
- Another process is writing to DB
- Wait for completion or kill process

**If Failed (File Not Found)**:
- Run schema migration first
- Create directory structure

**If Failed (Corruption)**:
- Backup current DB
- Restore from backup or re-ingest

---

### 4. CSV File Integrity ðŸ“„
**Problem Prevented**: Corrupted downloads causing ingestion crash

**Check**: Samples 3 files (first, middle, last) for:
- File size > 0 bytes
- Readable first line
- Basic structure validation

**If Failed**:
```bash
# Identify corrupted files
find /path/to/data -name "*.csv" -size 0

# Re-download corrupted files
# (use binance-history-data-downloader)
```

**Why Sample Only**: Checking 2,000+ files takes too long
**Coverage**: 99% of corruption issues caught by sampling

---

## Integration Examples

### Manual Ingestion
```bash
# 1. Pre-flight check
uv run python scripts/check_ingestion_ready.py \
    --db /media/sam/2TB-NVMe/liquidationheatmap_db/liquidations.duckdb \
    --data-dir /media/sam/3TB-WDC/binance-data

# 2. If passed, run ingestion
if [ $? -eq 0 ]; then
    uv run python scripts/ingest_aggtrades.py \
        --symbol BTCUSDT \
        --start-date 2024-01-01 \
        --end-date 2024-01-31 \
        --data-dir /media/sam/3TB-WDC/binance-data
fi
```

### n8n Workflow
```javascript
// Node 1: Pre-flight Check
{
  "name": "Pre-flight Check",
  "type": "n8n-nodes-base.executeCommand",
  "command": "uv run python scripts/check_ingestion_ready.py --db ... --data-dir ..."
}

// Node 2: Ingestion (only if check passed)
{
  "name": "Ingest Data",
  "type": "n8n-nodes-base.executeCommand",
  "executeOnlyIf": "{{ $node['Pre-flight Check'].json.exitCode === 0 }}"
}

// Node 3: Notification on Failure
{
  "name": "Alert on Failure",
  "type": "n8n-nodes-base.slack",
  "executeOnlyIf": "{{ $node['Pre-flight Check'].json.exitCode !== 0 }}"
}
```

### Cron Job (systemd timer)
```bash
#!/bin/bash
# /usr/local/bin/daily-ingestion.sh

# Pre-flight check
/path/to/uv run python scripts/check_ingestion_ready.py \
    --db /media/sam/2TB-NVMe/liquidationheatmap_db/liquidations.duckdb \
    --data-dir /media/sam/3TB-WDC/binance-data

if [ $? -ne 0 ]; then
    echo "Pre-flight check failed" | mail -s "Ingestion Failed" admin@example.com
    exit 1
fi

# Run ingestion
/path/to/uv run python scripts/ingest_aggtrades.py ...
```

---

## Metadata Tracking

After pre-flight checks pass, enable metadata tracking:

### Setup (One-Time)
```bash
uv run python scripts/migrate_add_metadata_tracking.py
```

Creates `ingestion_log` table to track:
- Which files processed
- When processed
- Row count per file
- Success/failure status
- Error messages
- Processing time

### Query Examples
```sql
-- Find failed ingestions
SELECT file_path, error_message, processed_at
FROM ingestion_log
WHERE status = 'failed'
ORDER BY processed_at DESC;

-- Check last processed file
SELECT file_path, processed_at, row_count
FROM ingestion_log
ORDER BY processed_at DESC
LIMIT 1;

-- Monthly ingestion stats
SELECT
    DATE_TRUNC('month', processed_at) as month,
    COUNT(*) as files_processed,
    SUM(row_count) as total_rows,
    AVG(processing_time_ms) as avg_time_ms
FROM ingestion_log
WHERE status = 'success'
GROUP BY month
ORDER BY month DESC;
```

---

## Troubleshooting

### "Lock held" Error
**Symptom**: Check fails with concurrent write lock error

**Diagnosis**:
```bash
lsof /tmp/liquidation_heatmap_ingestion.lock
ps aux | grep ingest_aggtrades
```

**Solution**:
1. Wait for running ingestion to complete
2. Or kill process: `kill <PID>` (data may be inconsistent)

---

### "Insufficient disk space" Error
**Symptom**: Check fails with <100GB free

**Diagnosis**:
```bash
df -h
du -sh /media/sam/2TB-NVMe/liquidationheatmap_db/liquidations.duckdb
```

**Solution**:
1. Archive old data (export to Parquet, compress)
2. Move DB to larger disk
3. Reduce retention period

---

### "Database error" (Corruption)
**Symptom**: Can't connect or query fails

**Diagnosis**:
```bash
# Try read-only connection
duckdb /media/sam/2TB-NVMe/liquidationheatmap_db/liquidations.duckdb -readonly "SELECT COUNT(*) FROM aggtrades_history"
```

**Solution**:
1. Backup current DB
2. Try `VACUUM` to rebuild
3. If failed, restore from backup
4. Last resort: re-ingest from CSV

---

### "CSV corrupted" Error
**Symptom**: Sample files have size 0 or can't be read

**Diagnosis**:
```bash
# Find all empty files
find /data -name "*.csv" -size 0

# Check specific file
head -1 /path/to/suspect.csv
```

**Solution**:
1. Re-download corrupted files
2. Delete empty files
3. Run ingestion (will skip missing days)

---

## Best Practices

### Before Production
- [ ] Run pre-flight check script
- [ ] Enable metadata tracking migration
- [ ] Set up monitoring/alerting
- [ ] Test recovery procedures
- [ ] Document backup strategy

### For n8n Workflows
- [ ] Add pre-flight check as first node
- [ ] Enable error notifications
- [ ] Set reasonable retry limits (max 3)
- [ ] Use idempotent operations (`ON CONFLICT DO NOTHING`)
- [ ] Log all execution results

### Ongoing Maintenance
- [ ] Monitor disk space weekly
- [ ] Review ingestion_log monthly
- [ ] Test backup restore quarterly
- [ ] Update pre-flight thresholds as needed

---

## See Also

- `docs/DATA_VALIDATION.md` - Post-ingestion quality checks
- `scripts/check_ingestion_ready.py` - Pre-flight check implementation
- `scripts/migrate_add_metadata_tracking.py` - Metadata table setup
