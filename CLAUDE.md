# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**ğŸ“˜ For Skill Implementation in Other Projects**: See `.claude/SKILLS_FRAMEWORK_BLUEPRINT.md` - Portable meta-framework for implementing Skills in ANY repository.

## Project Overview

**LiquidationHeatmap** calculates and visualizes cryptocurrency liquidation levels from Binance futures historical data. Uses DuckDB for fast CSV analytics, FastAPI for REST endpoints, and Redis pub/sub for real-time streaming. Leverages open-source models (py-liquidation-map) instead of reinventing algorithms.

**Key Principles**:
- KISS (Keep It Simple, Stupid) - Boring technology wins
- YAGNI (You Ain't Gonna Need It) - Build for today, not hypothetical futures
- Code Reuse First - Don't reinvent the wheel if >80% exists (py-liquidation-map formulas)
- Test-Driven Development - Red-Green-Refactor discipline

**ğŸ¯ Development Philosophy**:
â†’ The best code is no code. The second best is deleted code. The third best is simple code.

**Data Sources**:
- Binance historical CSV (3TB-WDC): trades, bookDepth, fundingRate, metrics (Open Interest)
- Real-time WebSocket (future): Binance Futures liquidation stream
- Target: BTC/USDT initially, expandable to other pairs

---

## Architecture

**3-Layer Design** (KISS approach - no custom binary parsing):

### Layer 1: Data Ingestion (DuckDB)
- **Raw Storage**: `data/raw/` (symlinked to Binance CSV - read-only)
- **Processed Storage**: `data/processed/*.duckdb` (analytics-optimized tables)
- **Pipeline**: Zero-copy CSV ingestion via `COPY FROM` (10GB in ~5 seconds)
- **Schema**: trades, liquidations, heatmap_cache (pre-computed aggregations)
- **Responsible Agent**: `data-engineer`

### Layer 2: Calculation & API (FastAPI + Redis)
- **Liquidation Models**: Binance-specific formulas (leverage py-liquidation-map)
  - Long liquidation: `entry_price * (1 - 1/leverage + maintenance_margin/leverage)`
  - Short liquidation: `entry_price * (1 + 1/leverage - maintenance_margin/leverage)`
- **Heatmap Generation**: Clustering algorithm from Open Interest + funding rates
- **REST API**: FastAPI endpoints (`/liquidations/heatmap`, `/historical`)
- **Streaming**: Redis pub/sub for real-time liquidation events (Nautilus pattern)
- **Responsible Agent**: `quant-analyst`

### Layer 3: Visualization (Plotly.js)
- **Heatmap**: Plotly.js scatter plot (price levels vs liquidation density)
- **Frontend**: Single HTML page (like UTXOracle - no build step)
- **API Client**: Fetch API for REST endpoints
- **Real-time**: WebSocket client for Redis pub/sub (future)

**Data Flow**:
```
Binance CSV (3TB-WDC)
  â†’ DuckDB ingestion (scripts/ingest_historical.py)
  â†’ Liquidation calculation (src/models/liquidation.py)
  â†’ Heatmap clustering (src/models/heatmap.py)
  â†’ REST API (api/main.py)
  â†’ Plotly.js visualization (frontend/heatmap.html)
```

### Tech Stack

| Layer | Technology | Justification |
|-------|------------|---------------|
| **Data Storage** | DuckDB | Zero-copy CSV, fast analytics, no server |
| **API Server** | FastAPI | Async, auto-docs, Pydantic integration |
| **Frontend** | Plotly.js | Lightweight, interactive visualizations |
| **Dependency Mgmt** | UV | 100x faster than pip, deterministic lockfiles |
| **Testing** | Pytest + TDD guard | Enforce test-first workflow |
| **Code Navigation** | Serena MCP | Fast codebase search |
| **Task Management** | SpecKit | Structured feature planning |

---

## Repository Organization

```
LiquidationHeatmap/
â”œâ”€â”€ src/                   # Core application code
â”œâ”€â”€ tests/                 # Test suite (pytest)
â”œâ”€â”€ scripts/               # Utilities and batch jobs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Symlink to external data (read-only)
â”‚   â”œâ”€â”€ processed/         # DuckDB databases (gitignored)
â”‚   â””â”€â”€ cache/             # Temporary cache (gitignored)
â”œâ”€â”€ frontend/              # Lightweight visualization
â”œâ”€â”€ .claude/               # Claude Code configuration
â”‚   â”œâ”€â”€ agents/            # Specialized subagents
â”‚   â”œâ”€â”€ skills/            # Template-driven automation
â”‚   â”œâ”€â”€ commands/          # SpecKit slash commands
â”‚   â”œâ”€â”€ prompts/           # Orchestration rules
â”‚   â””â”€â”€ tdd-guard/         # TDD enforcement
â”œâ”€â”€ .serena/               # Code navigation memory
â”œâ”€â”€ .specify/              # Task management
â”œâ”€â”€ CLAUDE.md              # THIS FILE
â”œâ”€â”€ README.md              # Public documentation
â”œâ”€â”€ pyproject.toml         # UV workspace root
â””â”€â”€ .gitignore             # Ignore patterns
```

### File Placement Conventions

**New backend code** â†’ `src/`
**New frontend code** â†’ `frontend/`
**Scripts/jobs** â†’ `scripts/`
**Tests** â†’ `tests/test_<module>.py`
**Documentation** â†’ `README.md` (public) or `.claude/docs/` (meta)
**Agent specs** â†’ `.claude/agents/`
**Skills** â†’ `.claude/skills/`

### Immutable Patterns

- **Never commit** `data/processed/` (DuckDB files are large)
- **Never commit** `.env` (secrets)
- **Always use** `uv` for dependency management (not `pip`)
- **Always test** before committing (`uv run pytest`)

---

## Development Principles

### ğŸ¯ KISS & YAGNI Blueprint (ALWAYS REMEMBER!)

#### **KISS** - Keep It Simple, Stupid
- **Choose boring technology**: Python, not Rust (until needed)
- **Avoid premature optimization**: Make it work, then make it fast
- **One module, one purpose**: Each file does ONE thing well
- **Minimize dependencies**: Every dependency is technical debt
- **Clear over clever**: Code that a junior can understand beats "smart" code

#### **YAGNI** - You Ain't Gonna Need It
- **Don't build for hypothetical futures**: Solve TODAY's problem
- **No unused abstractions**: 3 similar things â‰  need for abstraction yet
- **Delete dead code**: If unused for 2 weeks, remove it
- **Resist feature creep**: Solve the core problem first

#### **Code Reuse First** - Don't Reinvent the Wheel
- **NEVER write custom code if >80% can be reused**: Use existing libraries
- **Analyze before automating**: Verify existing solutions don't already solve it
- **Leverage open source**: Battle-tested code > custom implementation

---

## ğŸ”§ Development Workflow

### TDD Implementation Flow

**Red-Green-Refactor** (enforced by TDD guard):

1. **ğŸ”´ RED**: Write failing test first
   ```bash
   uv run pytest tests/test_module.py::test_new_feature -v  # MUST fail
   git add tests/ && git commit -m "TDD RED: Add test for feature X"
   ```

2. **ğŸŸ¢ GREEN - BABY STEPS** (critical - TDD guard enforces this):

   **Step 2a**: Add MINIMAL stub (just method signature)
   ```python
   def new_method(self):
       """Stub - not implemented yet"""
       raise NotImplementedError
   ```
   Run test â†’ Should fail differently

   **Step 2b**: Add MINIMAL implementation
   ```python
   def new_method(self):
       """Minimal implementation to pass test"""
       return []  # Simplest return value
   ```
   Run test â†’ May still fail on assertions

   **Step 2c**: Iterate until GREEN
   ```bash
   uv run pytest tests/test_module.py::test_new_feature -v  # Should pass
   git add . && git commit -m "TDD GREEN: Implement feature X"
   ```

3. **â™»ï¸ REFACTOR**: Clean up with tests passing
   ```bash
   uv run pytest  # All tests still pass
   git add . && git commit -m "TDD REFACTOR: Clean up feature X"
   ```

**âš ï¸ TDD Guard Rules** (enforced automatically):
- âŒ **NEVER** implement without failing test first
- âŒ **NEVER** add multiple tests at once (one test at a time)
- âœ… **ALWAYS** run pytest immediately before AND after each edit
- âœ… **ALWAYS** implement smallest possible change

---

### When Stuck Protocol

**CRITICAL**: Maximum **3 attempts** per issue, then STOP.

#### After 3 Failed Attempts:

1. **Document failure** in issue/PR
2. **Research alternatives** (15min max) - Find 2-3 similar implementations
3. **Question fundamentals**: Is this the right approach? Can it be split?
4. **Try different angle OR ask for help**

**Never**: Keep trying the same approach >3 times.

---

### Decision Framework

When multiple valid approaches exist, choose based on **priority order**:

1. **Testability** â†’ Can I easily test this?
2. **Simplicity** â†’ Is this the simplest solution that works?
3. **Consistency** â†’ Does this match existing project patterns?
4. **Readability** â†’ Will someone understand this in 6 months?
5. **Reversibility** â†’ How hard to change later?

---

### Error Handling Standards

**Principles**:
- **Fail fast** with descriptive messages
- **Include context** for debugging
- **Never silently swallow** exceptions

**Good Error Messages**:
```python
# âŒ Bad
raise ValueError("Invalid input")

# âœ… Good
raise ValueError(
    f"DuckDB connection failed: {db_path} "
    f"(check file exists and has read permissions)"
)
```

---

### Test Guidelines

**Principles**:
- Test **behavior**, not implementation
- **One assertion** per test when possible
- **Clear test names**: `test_<what>_<when>_<expected>`
- Tests must be **deterministic** (no random, no time dependencies)

**Good Test Structure**:
```python
def test_liquidation_calculation_returns_correct_price_when_leverage_10x():
    """Liquidation price should be 90% of entry for 10x long position."""
    # Arrange
    entry_price = 100.0
    leverage = 10

    # Act
    liq_price = calculate_liquidation_price(entry_price, leverage, "long")

    # Assert
    assert liq_price == 90.0
```

---

## Agent & Skill Architecture

### **Subagents** - Complex Reasoning
Specialized agents for deep domain expertise and multi-step workflows.

| Agent | Responsibility | Key Tasks |
|-------|---------------|-----------|
| **data-engineer** | DuckDB ingestion, ETL pipelines | CSV â†’ DuckDB, schema design, data quality checks |
| **quant-analyst** | Liquidation models, heatmaps, backtesting | Formula implementation, clustering algorithms, strategy validation |

**Full specs**: See `.claude/agents/` directory

**Usage**: Invoke via Claude Code for complex implementation tasks.

### **Skills** - Template-Driven Automation
Lightweight templates for repetitive operations with 60-83% token savings.

| Skill | Purpose | Token Savings | Status |
|-------|---------|---------------|--------|
| pytest-test-generator | Auto-generate test boilerplate | 83% (3,000â†’500) | âœ… |
| github-workflow | PR/Issue/Commit templates | 79% (18,900â†’4,000) | âœ… |
| pydantic-model-generator | Pydantic data models with validators | 75% (2,000â†’500) | âœ… |

**Usage**: Automatically triggered by keywords (see `.claude/skills/*/SKILL.md`)

---

## Important Reminders

### âŒ **NEVER**:
- Use `--no-verify` to bypass commit hooks
- Disable tests instead of fixing them
- Commit code that doesn't compile/run
- Hardcode secrets/API keys (use `.env`)
- Commit without testing locally first

### âœ… **ALWAYS**:
- Run tests before committing (`uv run pytest`)
- Format/lint before committing (`ruff check . && ruff format .`)
- Write commit message explaining **WHY** (not just what)
- Update relevant docs when changing behavior
- Use `uv` for dependencies (not `pip`)

---

## ğŸ§¹ Task Completion Protocol

**Before marking ANY task complete or creating commit**:

### âœ… Pre-Commit Cleanup Checklist

1. **Remove temporary files**: `find . -name "*.tmp" -o -name "*.bak"`
2. **Clean Python cache**: `find . -type d -name "__pycache__" -exec rm -rf {} +`
3. **Remove debug code**: Check for `print()`, `console.log`, `import pdb`
4. **Run linter**: `ruff check . && ruff format .`
5. **Run tests**: `uv run pytest`
6. **Check git status**: No uncommitted temp files

### ğŸ—‘ï¸ What to DELETE vs KEEP

**DELETE**:
- Temporary files (`.tmp`, `.bak`)
- Python cache (`__pycache__`, `.pyc`)
- Debug logs
- Commented code blocks >1 week old
- Unused imports

**KEEP**:
- Source code in `src/`, `tests/`, `scripts/`
- Configuration files
- Documentation (if referenced)
- `uv.lock` (dependency lockfile - COMMIT THIS!)

---

## Known Models & References

**Leverage existing open-source work** (KISS - don't reinvent):

1. **py-liquidation-map** (GitHub: aoki-h-jp/py-liquidation-map)
   - Liquidation clustering algorithm
   - Heatmap visualization approach
   - Supports Binance + Bybit data

2. **binance-liquidation-tracker** (GitHub: hgnx/binance-liquidation-tracker)
   - Real-time WebSocket tracking
   - Forced liquidation monitoring

3. **Coinglass formulas** (industry standard)
   - Liquidation heatmap calculations
   - Open Interest analysis

4. **Binance Official Docs**
   - Liquidation formula: https://www.binance.com/en/support/faq/liquidation
   - Maintenance margin rates (varies by leverage)

**Strategy**: Use py-liquidation-map formulas as reference implementation, adapt for DuckDB vectorized calculations.

---

## License

MIT License (or specify alternative)
