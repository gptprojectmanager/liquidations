#!/usr/bin/env python3
"""Complete end-to-end aggTrades orchestrator - N8N PRODUCTION READY.

Features:
1. Creates database schema if not exists
2. Auto-detects and fills gaps
3. Full ingestion mode for initial bulk load
4. Plain text output (N8N compatible)
5. Idempotent and safe to re-run (INSERT OR IGNORE)

Modes:
- auto: Gap detection and filling (incremental updates)
- full: Load all files in date range (initial bulk ingestion)
- dry-run: Validation only, no data loaded
"""

import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

import duckdb

# Adjust path for imports
sys.path.insert(0, "/media/sam/1TB/LiquidationHeatmap")

from src.liquidationheatmap.ingestion.aggtrades_streaming import load_aggtrades_streaming

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CompleteIngestionOrchestrator:
    """Complete end-to-end orchestrator with schema creation."""

    def __init__(self, symbol: str, data_dir: Path, db_path: Path, throttle_ms: int = 200,
                 start_date: str = None, end_date: str = None):
        self.symbol = symbol
        self.data_dir = data_dir
        self.db_path = db_path
        self.throttle_ms = throttle_ms
        self.start_date = start_date
        self.end_date = end_date
        self.conn = None

    def connect(self):
        """Open database connection."""
        self.conn = duckdb.connect(str(self.db_path))
        logger.info(f"Connected to database: {self.db_path}")

    def close(self):
        """Close database connection."""
        if self.conn:
            self.conn.close()

    def ensure_schema(self):
        """Create table and indexes if they don't exist."""
        print("\nüîß Phase 0: Schema Validation")
        
        try:
            # Check if table exists
            result = self.conn.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_name = 'aggtrades_history'
            """).fetchone()
            
            if result[0] == 0:
                print("‚ö†Ô∏è  Table does not exist - creating schema...")
                
                # Create table with PRIMARY KEY
                self.conn.execute("""
                    CREATE TABLE aggtrades_history (
                        agg_trade_id BIGINT PRIMARY KEY,
                        timestamp TIMESTAMP NOT NULL,
                        symbol VARCHAR NOT NULL,
                        price DECIMAL(18, 8) NOT NULL,
                        quantity DECIMAL(18, 8) NOT NULL,
                        side VARCHAR NOT NULL,
                        gross_value DOUBLE NOT NULL
                    )
                """)
                print("‚úÖ Table created with PRIMARY KEY on agg_trade_id")
                
                # Create indexes
                self.conn.execute(
                    "CREATE INDEX idx_aggtrades_timestamp_symbol ON aggtrades_history(timestamp, symbol)"
                )
                self.conn.execute(
                    "CREATE INDEX idx_aggtrades_timestamp ON aggtrades_history(timestamp)"
                )
                print("‚úÖ Indexes created")
            else:
                # Verify PRIMARY KEY exists
                pk_check = self.conn.execute("""
                    SELECT constraint_type 
                    FROM information_schema.table_constraints 
                    WHERE table_name = 'aggtrades_history' AND constraint_type = 'PRIMARY KEY'
                """).fetchall()
                
                if pk_check:
                    print("‚úÖ Schema validated (PRIMARY KEY exists)")
                else:
                    print("‚ö†Ô∏è  WARNING: Table exists but has NO PRIMARY KEY!")
                    print("   Run /tmp/rebuild_db_with_pk_auto.py to fix this!")
                    return False
                    
        except Exception as e:
            print(f"‚ùå Schema error: {e}")
            return False
            
        return True

    def discover_files(self) -> List[Path]:
        """Phase 1: Discover available CSV files."""
        print("\nüìÇ Phase 1: Discovery")

        aggtrades_dir = self.data_dir / self.symbol / "aggTrades"
        if not aggtrades_dir.exists():
            print(f"‚ùå Directory not found: {aggtrades_dir}")
            return []

        all_files = sorted(aggtrades_dir.glob(f"{self.symbol}-aggTrades-*.csv"))
        print(f"‚úÖ Found {len(all_files):,} CSV files")

        return all_files

    def detect_gaps(self) -> List[Tuple[str, str]]:
        """Phase 2: Detect missing date ranges in database."""
        print("\nüîç Phase 2: Gap Detection")

        try:
            result = self.conn.execute("""
                SELECT MIN(DATE(timestamp)), MAX(DATE(timestamp))
                FROM aggtrades_history
            """).fetchone()

            if not result[0]:
                print("‚ö†Ô∏è  Empty database - will ingest all available data")
                return []

            min_date, max_date = result

            gaps_query = f"""
                WITH RECURSIVE date_series AS (
                    SELECT DATE '{min_date}' as expected_date
                    UNION ALL
                    SELECT expected_date + INTERVAL 1 DAY
                    FROM date_series
                    WHERE expected_date < DATE '{max_date}'
                ),
                actual_dates AS (
                    SELECT DISTINCT DATE(timestamp) as actual_date
                    FROM aggtrades_history
                )
                SELECT d.expected_date
                FROM date_series d
                LEFT JOIN actual_dates a ON d.expected_date = a.actual_date
                WHERE a.actual_date IS NULL
                ORDER BY d.expected_date
            """

            missing_dates = [row[0] for row in self.conn.execute(gaps_query).fetchall()]

            if not missing_dates:
                print("‚úÖ No gaps detected")
                return []

            gaps = self._consolidate_gaps(missing_dates)
            print(f"‚ö†Ô∏è  Found {len(gaps)} gap(s):")
            for start, end in gaps:
                days = (end - start).days + 1
                print(f"   ‚Ä¢ {start} ‚Üí {end} ({days} days)")

            return gaps
            
        except Exception as e:
            logger.error(f"Gap detection failed: {e}")
            return []

    def _consolidate_gaps(self, missing_dates: List) -> List[Tuple[str, str]]:
        """Consolidate consecutive missing dates into ranges."""
        if not missing_dates:
            return []

        gaps = []
        range_start = missing_dates[0]
        prev_date = missing_dates[0]

        for current_date in missing_dates[1:]:
            if (current_date - prev_date).days > 1:
                gaps.append((str(range_start), str(prev_date)))
                range_start = current_date
            prev_date = current_date

        gaps.append((str(range_start), str(prev_date)))
        return gaps

    def fill_gaps(self, gaps: List[Tuple[str, str]], max_retries: int = 3) -> bool:
        """Phase 3: Fill detected gaps with retry logic."""
        if not gaps:
            return True

        print(f"\nüîß Phase 3: Gap Filling ({len(gaps)} gap(s))")

        success_count = 0
        fail_count = 0

        for idx, (start_date, end_date) in enumerate(gaps, 1):
            print(f"\nGap {idx}/{len(gaps)}: {start_date} ‚Üí {end_date}")

            for attempt in range(1, max_retries + 1):
                try:
                    total_rows = load_aggtrades_streaming(
                        self.conn,
                        self.data_dir,
                        self.symbol,
                        start_date,
                        end_date,
                        throttle_ms=self.throttle_ms
                    )

                    print(f"‚úÖ Filled: {total_rows:,} rows")
                    success_count += 1
                    break

                except Exception as e:
                    if attempt < max_retries:
                        print(f"‚ö†Ô∏è  Attempt {attempt} failed: {e}")
                    else:
                        print(f"‚ùå Failed after {max_retries} attempts: {e}")
                        fail_count += 1

        print(f"\nüìä Gap filling: ‚úÖ {success_count}/{len(gaps)} ‚ùå {fail_count}/{len(gaps)}")
        return fail_count == 0

    def generate_report(self) -> dict:
        """Phase 4: Generate final summary report."""
        print("\nüìä Phase 4: Final Report")

        stats = self.conn.execute("""
            SELECT
                COUNT(*) as total_rows,
                MIN(timestamp) as min_timestamp,
                MAX(timestamp) as max_timestamp,
                COUNT(DISTINCT DATE(timestamp)) as days_with_data,
                MIN(price) as min_price,
                MAX(price) as max_price,
                SUM(gross_value) as total_volume
            FROM aggtrades_history
        """).fetchone()

        total_rows, min_ts, max_ts, days, min_price, max_price, volume = stats

        print("=" * 70)
        print("  INGESTION SUMMARY")
        print("=" * 70)
        print(f"Total Rows:     {total_rows:,}")
        print(f"Date Range:     {min_ts} ‚Üí {max_ts}")
        print(f"Days with Data: {days:,}")
        print(f"Avg Rows/Day:   {total_rows // days:,}" if days > 0 else "Avg Rows/Day:   N/A")
        print(f"Price Range:    ${min_price:,.2f} - ${max_price:,.2f}")
        print(f"Total Volume:   ${volume:,.2f}")
        print("=" * 70)

        # Final gap check
        gaps = self.detect_gaps()
        if gaps:
            print(f"\n‚ö†Ô∏è  WARNING: {len(gaps)} gap(s) remaining!")
            for start, end in gaps:
                print(f"   ‚Ä¢ {start} ‚Üí {end}")
        else:
            print("\n‚úÖ No gaps detected - data is continuous")

        return {"total_rows": total_rows, "gaps": len(gaps)}

    def run(self, mode: str = "auto") -> int:
        """Main orchestration workflow."""
        print("\n" + "=" * 70)
        print("  COMPLETE INGESTION ORCHESTRATOR (N8N-READY)")
        print("=" * 70)
        print(f"Symbol: {self.symbol}")
        print(f"Mode: {mode}")
        print(f"Throttle: {self.throttle_ms}ms")
        if self.start_date and self.end_date:
            print(f"Date Range: {self.start_date} ‚Üí {self.end_date}")

        try:
            self.connect()

            # Phase 0: Ensure schema exists
            if not self.ensure_schema():
                print("\n‚ùå Schema validation failed")
                return 1

            # Phase 1: Discovery
            files = self.discover_files()
            if not files:
                print("\n‚ö†Ô∏è  No files found")
                return 0

            if mode == "dry-run":
                print("\n‚úÖ Dry run complete")
                return 0

            # MODE: FULL - Load all files in date range (initial bulk ingestion)
            if mode == "full":
                if not self.start_date or not self.end_date:
                    print("\n‚ùå --start-date and --end-date required for --mode full")
                    return 1

                print(f"\nüöÄ Phase 2: Full Ingestion Mode")
                print(f"Loading ALL files from {self.start_date} to {self.end_date}")
                print("(Duplicates automatically ignored via INSERT OR IGNORE)")

                try:
                    total_rows = load_aggtrades_streaming(
                        self.conn,
                        self.data_dir,
                        self.symbol,
                        self.start_date,
                        self.end_date,
                        throttle_ms=self.throttle_ms
                    )
                    print(f"‚úÖ Loaded: {total_rows:,} rows")

                except Exception as e:
                    print(f"‚ùå Full ingestion failed: {e}")
                    logger.exception("Full ingestion failed")
                    return 1

                # Phase 4: Report
                self.generate_report()
                print("\n‚úÖ Full ingestion complete!")
                return 0

            # MODE: AUTO - Gap detection and filling (incremental updates)
            # Phase 2-3: Detect and fill gaps
            gaps = self.detect_gaps()
            if gaps and mode == "auto":
                success = self.fill_gaps(gaps)
                if not success:
                    print("\n‚ö†Ô∏è  Some gaps could not be filled")
                    # Don't return 1 - continue to report
            elif gaps and mode != "auto":
                print(f"\n‚ö†Ô∏è  {len(gaps)} gaps detected but not filling (mode={mode})")

            # Phase 4: Report
            self.generate_report()

            print("\n‚úÖ Orchestration complete!")
            return 0

        except Exception as e:
            print(f"\n‚ùå Fatal error: {e}")
            logger.exception("Orchestration failed")
            return 1

        finally:
            self.close()


def main():
    parser = argparse.ArgumentParser(
        description="Complete aggTrades orchestrator (N8N-ready)",
        epilog="""
Examples:
  # Gap filling (daily incremental updates)
  python3 ingest_full_history_n8n.py --data-dir /path/to/data --mode auto

  # Full ingestion (initial bulk load)
  python3 ingest_full_history_n8n.py --data-dir /path/to/data --mode full \\
    --start-date 2021-12-01 --end-date 2025-11-01

  # Dry run (validation only)
  python3 ingest_full_history_n8n.py --data-dir /path/to/data --mode dry-run
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--symbol", default="BTCUSDT", help="Trading pair (default: BTCUSDT)")
    parser.add_argument("--data-dir", required=True, help="Base data directory containing CSV files")
    parser.add_argument("--db", default="/media/sam/1TB/LiquidationHeatmap/data/processed/liquidations.duckdb",
                       help="DuckDB database path")
    parser.add_argument("--mode", default="auto", choices=["auto", "full", "dry-run"],
                       help="Ingestion mode: auto (gaps), full (bulk load), dry-run (validate)")
    parser.add_argument("--start-date", help="Start date for full mode (YYYY-MM-DD, e.g., 2021-12-01)")
    parser.add_argument("--end-date", help="End date for full mode (YYYY-MM-DD, e.g., 2025-11-01)")
    parser.add_argument("--throttle-ms", type=int, default=200,
                       help="I/O throttle between files in milliseconds (default: 200ms)")

    args = parser.parse_args()

    # Validate: full mode requires date range
    if args.mode == "full" and (not args.start_date or not args.end_date):
        parser.error("--mode full requires both --start-date and --end-date")

    orchestrator = CompleteIngestionOrchestrator(
        symbol=args.symbol,
        data_dir=Path(args.data_dir),
        db_path=Path(args.db),
        throttle_ms=args.throttle_ms,
        start_date=args.start_date,
        end_date=args.end_date
    )

    sys.exit(orchestrator.run(mode=args.mode))


if __name__ == "__main__":
    main()
