# LiquidationHeatmap

Calculate and visualize cryptocurrency liquidation levels from Binance futures data using DuckDB analytics and FastAPI REST endpoints. Leverages open-source models (py-liquidation-map) for battle-tested algorithms.

## Quick Start

```bash
# Install dependencies
uv sync

# Pre-flight checks (recommended for production)
uv run python scripts/check_ingestion_ready.py \
    --db data/processed/liquidations.duckdb \
    --data-dir /path/to/binance-data

# Ingest historical CSV data (example: Jan 2025)
uv run python scripts/ingest_aggtrades.py \
    --symbol BTCUSDT \
    --start-date 2025-01-01 \
    --end-date 2025-01-31 \
    --data-dir /path/to/binance-data

# Validate data quality (after ingestion)
uv run python scripts/validate_aggtrades.py

# Run FastAPI server
uv run uvicorn api.main:app --reload

# Open visualization
open http://localhost:8000/heatmap.html

# Run tests
uv run pytest
```

## Architecture

**3-Layer Design**:
1. **Data**: DuckDB (zero-copy CSV ingestion, fast analytics)
2. **API**: FastAPI (REST endpoints) + Redis (pub/sub streaming)
3. **Viz**: Plotly.js (interactive heatmaps)

See `CLAUDE.md` for detailed architecture and development workflow.

## Data Sources

- **Raw CSV**: `data/raw/BTCUSDT/` (symlinked to Binance historical data on 3TB-WDC)
  - trades/, bookDepth/, fundingRate/, metrics/ (Open Interest)
- **Processed**: `data/processed/*.duckdb` (analytics-optimized tables)
- **Cache**: `data/cache/` (Redis snapshots, temporary files)

**Note**: Raw data is read-only (symlinked). All analytics use DuckDB as single source of truth.

## Development

### Setup

```bash
# Clone repository
git clone <repo-url>
cd LiquidationHeatmap

# Install dependencies
uv sync

# Copy environment template
cp .env.example .env
# Edit .env with your configuration
```

### Testing

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=src --cov-report=html

# Run specific test
uv run pytest tests/test_module.py::test_function
```

### TDD Workflow

This project uses Test-Driven Development (TDD):

1. **RED**: Write failing test first
2. **GREEN**: Write minimal code to pass test
3. **REFACTOR**: Clean up code while tests pass

See `CLAUDE.md` for detailed TDD workflow.

## Data Validation

After ingestion, validate data quality with:

```bash
uv run python scripts/validate_aggtrades.py
```

**Validation checks**:
- Basic statistics (row count, date range, price range)
- Duplicate detection
- Invalid values (negative prices, NULL fields)
- Temporal continuity (gap detection)
- Sanity checks (realistic value ranges)

See `docs/DATA_VALIDATION.md` for detailed documentation.

## Project Structure

```
LiquidationHeatmap/
â”œâ”€â”€ src/              # Core application code
â”œâ”€â”€ tests/            # Test suite
â”œâ”€â”€ scripts/          # Utilities and batch jobs
â”‚   â”œâ”€â”€ ingest_aggtrades.py        # Streaming ingestion
â”‚   â”œâ”€â”€ check_ingestion_ready.py   # Pre-flight checks (production)
â”‚   â”œâ”€â”€ validate_aggtrades.py      # Data quality validation
â”‚   â”œâ”€â”€ migrate_add_unique_constraint.py     # Duplicate prevention
â”‚   â””â”€â”€ migrate_add_metadata_tracking.py     # Metadata logging
â”œâ”€â”€ docs/             # Documentation
â”‚   â”œâ”€â”€ DATA_VALIDATION.md         # Validation guide
â”‚   â””â”€â”€ PRODUCTION_CHECKLIST.md    # Production readiness
â”œâ”€â”€ data/             # Data directory
â”‚   â”œâ”€â”€ raw/          # External data (symlink)
â”‚   â”œâ”€â”€ processed/    # DuckDB databases
â”‚   â””â”€â”€ cache/        # Temporary cache
â”œâ”€â”€ frontend/         # Visualization (if applicable)
â”œâ”€â”€ CLAUDE.md         # Development guide for Claude Code
â”œâ”€â”€ README.md         # This file
â””â”€â”€ pyproject.toml    # Dependencies (UV)
```

## Contributing

1. Follow TDD workflow (see `CLAUDE.md`)
2. Run tests before committing
3. Format code with `ruff format .`
4. Lint code with `ruff check .`
5. Write clear commit messages (explain WHY, not just WHAT)

## Key Features

- âœ… **Zero-copy CSV ingestion**: DuckDB loads 10GB in ~5 seconds
- âœ… **Binance liquidation formulas**: Leverage py-liquidation-map algorithms
- âœ… **Real-time streaming**: Redis pub/sub (Nautilus pattern)
- âœ… **Interactive heatmaps**: Plotly.js visualization (no build step)
- âœ… **Test-Driven Development**: TDD guard enforces 80% coverage

## References

- [py-liquidation-map](https://github.com/aoki-h-jp/py-liquidation-map) - Liquidation clustering
- [binance-liquidation-tracker](https://github.com/hgnx/binance-liquidation-tracker) - Real-time tracking
- [Binance Liquidation Guide](https://www.binance.com/en/support/faq/liquidation) - Official formulas

## License

MIT License

## API Endpoints

### Base URL
```
http://localhost:8000
```

### Available Endpoints

#### 1. Health Check
```bash
GET /health
```
Returns API status.

#### 2. Liquidation Levels
```bash
GET /liquidations/levels?symbol=BTCUSDT&model=binance_standard
```
**Parameters**:
- `symbol`: Trading pair (default: BTCUSDT)
- `model`: Model type (`binance_standard` | `ensemble`)

**Returns**: Long liquidations (below price) and short liquidations (above price).

**Example**:
```bash
curl "http://localhost:8000/liquidations/levels?symbol=BTCUSDT&model=ensemble"
```

#### 3. Historical Liquidations
```bash
GET /liquidations/history?symbol=BTCUSDT&aggregate=true&start=2024-10-29T18:00:00
```
**Parameters**:
- `symbol`: Trading pair (default: BTCUSDT)
- `aggregate`: Group by timestamp and side (default: false)
- `start`: Start datetime (ISO format, optional)
- `end`: End datetime (ISO format, optional)

**Returns**: Historical liquidation records or aggregated data.

**Examples**:
```bash
# Aggregated data for time-series
curl "http://localhost:8000/liquidations/history?symbol=BTCUSDT&aggregate=true"

# Raw records with date filtering
curl "http://localhost:8000/liquidations/history?symbol=BTCUSDT&start=2024-10-01&end=2024-10-31"
```

#### 4. Liquidation Heatmap
```bash
GET /liquidations/heatmap?symbol=BTCUSDT&model=binance_standard
```
**Parameters**:
- `symbol`: Trading pair (default: BTCUSDT)
- `model`: Model type (`binance_standard` | `ensemble`)
- `timeframe`: Time bucket (1h|4h|12h|1d|7d|30d, default: 1d)

**Returns**: Pre-aggregated heatmap data with density and volume per time+price bucket.

**Example**:
```bash
curl "http://localhost:8000/liquidations/heatmap?symbol=BTCUSDT&model=ensemble"
```

## Frontend Visualizations

### 1. Liquidation Map
```bash
open frontend/liquidation_map.html
```
Bar chart showing liquidation levels by price and leverage tier (Coinglass-style).

### 2. Historical Liquidations
```bash
open frontend/historical_liquidations.html
```
Time-series chart of liquidation volume over time with dual-axis (longs/shorts).

### 3. Liquidation Heatmap
```bash
open frontend/heatmap.html
```
2D heatmap (time Ã— price) showing liquidation density with color gradient.

## Features

âœ… **Liquidation Models**:
- Binance Standard (95% accuracy)
- Funding-Adjusted (experimental)
- Ensemble (weighted average)

âœ… **Data Ingestion**:
- DuckDB zero-copy CSV loading (<5s per 10GB)
- Open Interest & Funding Rate tracking
- Data validation & quality checks

âœ… **API**:
- FastAPI REST endpoints
- Retry logic with exponential backoff
- Structured logging to `logs/liquidationheatmap.log`

âœ… **Visualization**:
- Plotly.js interactive charts
- Coinglass color scheme (#d9024b, #45bf87, #f0b90b)
- Responsive design (mobile + desktop)

## Testing

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=src --cov-report=html

# Open coverage report
open htmlcov/index.html
```

**Test Coverage**: 36% (target: â‰¥80%)

## Project Status

**Completed** (37/51 tasks, 73%):
- âœ… Phase 1: Setup
- âœ… Phase 2: Data Layer  
- âœ… Phase 3: Liquidation Calculation (MVP)
- âœ… Phase 4: Visualization (88%)
- âœ… Phase 7: Polish (retry, logging, tests)

**Pending**:
- â³ Phase 5: Model Comparison (US3)
- ðŸ”® Phase 6: Nautilus Integration (US4, future)

See `.specify/tasks.md` for detailed task list.

## License

MIT License
